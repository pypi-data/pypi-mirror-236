{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from time import perf_counter\n",
    "import dsds.fs as fs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_x, orig_y = make_classification(n_samples = 50_000, n_features = 500, n_informative = 60, n_redundant = 440)\n",
    "# This is a Polars dataframe. This is dsds package's favored dataframe. dsds relies on Polars heavily.\n",
    "# You must turn other dataframe formats into Polars for dsds to work.\n",
    "df = pl.from_numpy(orig_x).insert_at_idx(0, pl.Series(\"target\", orig_y)) \n",
    "# Turn it into Pandas.\n",
    "df_pd = df.to_pandas()\n",
    "target = \"target\"\n",
    "features = df.columns\n",
    "features.remove(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons\n",
    "\n",
    "This notebook compares results and performance between the dsds package, sklearn and some other packages for feature selection and some other transformations common in the data science pipeline.\n",
    "\n",
    "### Methods Compared:\n",
    "1. Scaling and Imputation\n",
    "2. Fscore\n",
    "3. Mutual Information Score\n",
    "4. MRMR feature selection strategies\n",
    "5. Power Transform\n",
    "\n",
    "You may restart the kernel after each section. But remember to rerun the cells above. If you are concerned about memory usage when running this notebook, go to the end and run the gc cell."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling and Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dsds.transform as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns\n",
    "features.remove(\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = t.scale(df, cols=features, strategy=\"standard\")\n",
    "scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The difference in result is caused by using ddof = 1 for sample variance in dsds\n",
    "# and using ddof = 0 in sklearn.\n",
    "\n",
    "# Long and convoluted code just to do some scaling...\n",
    "std = StandardScaler()\n",
    "scaled2 = std.fit_transform(df_pd[features], df_pd[target])\n",
    "# scaled2[:5, :] # scaled2 is a numpy matrix\n",
    "scaled2 = pd.DataFrame(scaled2, columns=features)\n",
    "scaled2[target] = df_pd[target]\n",
    "scaled2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "scaled = t.scale(df, cols=features, strategy=\"standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "std = StandardScaler()\n",
    "scaled2 = std.fit_transform(df_pd[features], df_pd[target])\n",
    "scaled2 = pd.DataFrame(scaled2, columns=features)\n",
    "scaled2[target] = df_pd[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dsds.transform as t\n",
    "t.impute(df, cols=features, strategy=\"median\").head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer \n",
    "imputer = SimpleImputer(strategy = \"median\")\n",
    "imputed = pd.DataFrame(imputer.fit_transform(df_pd, df_pd[target]), columns=df.columns)\n",
    "imputed.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "imputer = SimpleImputer(strategy = \"median\")\n",
    "imputed = pd.DataFrame(imputer.fit_transform(df_pd, df_pd[target]), columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "t.impute(df, cols=features, strategy=\"median\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dsds.fs as fs # fs = feature_selection\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif, f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs._f_score(df, target=target, num_list = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The more core, the bigger the difference. Data here is not big enough to show the difference\n",
    "start = perf_counter()\n",
    "res = fs.f_classif(df, target=target)\n",
    "end = perf_counter()\n",
    "print(f\"Spent {end - start:.2f}s in computing Fscore.\")\n",
    "res.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "f, pv = f_classif(df_pd[features], df_pd[target])\n",
    "res = pd.DataFrame({\"feature\":features, \"f_value\":f, \"p_value\":pv})\n",
    "end = perf_counter()\n",
    "print(f\"Spent {end - start:.2f}s in computing Fscore.\")\n",
    "res.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "f, pv = f_regression(df_pd[features], df_pd[target])\n",
    "res = pd.DataFrame({\"feature\":features, \"f_value\":f, \"p_value\":pv})\n",
    "end = perf_counter()\n",
    "print(f\"Spent {end - start: .2f}s in computing Fscore.\")\n",
    "res.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Information Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vastly faster than sklearn. Finished in 0.7s in this run\n",
    "fs.mutual_info(df, target=target, conti_cols=features).sort(by=\"estimated_mi\", descending=True).limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A wrapper for more apples to apples comparison\n",
    "def estimate_mi_sklearn(df:pd.DataFrame, cols:list[str], target:str, k=3, random_state:int=42):\n",
    "    mi_estimates = mutual_info_classif(df[cols], df[target]\n",
    "                        , n_neighbors=k, random_state=random_state, discrete_features=False)\n",
    "\n",
    "    return pl.from_records([cols, mi_estimates], schema=[\"feature\", \"estimated_mi\"]).sort(\"estimated_mi\", descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason sklearn's impl is slow is that it did not turn on multithreading for KDtrees.\n",
    "# Sklearn also did not provide an option to turn it on, despite the fact that sklearn's KDtrees\n",
    "# does have this functionality. Finished in 4.4s in this run\n",
    "estimate_mi_sklearn(df_pd, cols=features, target=target).limit(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRMR Feature selection Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrmr import mrmr_classif # This is currently the most starred MRMR Python package on github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to wrap it so that we get apples to apples comparison\n",
    "def mrmr_package(df:pd.DataFrame, target:str, k:int) -> list[str]:\n",
    "    features = list(df.columns)\n",
    "    features.remove(target)\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    start = perf_counter()\n",
    "    output = mrmr_classif(X, y, K = k)\n",
    "    end = perf_counter()\n",
    "    print(f\"Spent {end - start:.2f}s to compute mrmr.\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = mrmr_package(df_pd, \"target\", 50)\n",
    "out1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "# mrmr from dsds package. This is actually a slow version, but is much less memory intensive \n",
    "# and is more practical for real life data. See docstring for more details.\n",
    "out2 = fs.mrmr(df, target=\"target\", k = 50)\n",
    "end = perf_counter()\n",
    "print(f\"Spent {end - start:.2f}s in computing.\")\n",
    "out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 == out2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSDS packages provides another kind of MRMR\n",
    "fs.knock_out_mrmr(df, target=\"target\", k = 50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "fs.knock_out_mrmr(df, target=\"target\", k = 50) \n",
    "end = perf_counter()\n",
    "print(f\"Spent {end - start:.2f}s in computing.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "res_eager = t.power_transform(df, cols=features, strategy=\"yeo_johnson\")\n",
    "end = perf_counter()\n",
    "print(f\"Spent {end - start:.2f}s in computing.\")\n",
    "res_eager.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import power_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn with Pandas\n",
    "\n",
    "start = perf_counter()\n",
    "transformed = power_transform(df_pd[features], method = \"yeo-johnson\", standardize=False)\n",
    "end = perf_counter()\n",
    "df_pd[features] = transformed\n",
    "print(f\"Spent {end - start:.2f}s in computing.\")\n",
    "df_pd.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
