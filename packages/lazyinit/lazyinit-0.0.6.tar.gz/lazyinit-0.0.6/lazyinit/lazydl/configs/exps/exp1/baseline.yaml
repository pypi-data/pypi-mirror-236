# @package _global_

# Section 关于实验的标记
comet_project: focusl
comet_name: t5_baseline # 对本次实验的简短描述
memo: FaithDial数据集，T5 # 对本次实验的详细描述，可以用来记录本次实验的具体细节和改动
proc_title: ${comet_name} # 修改后的进程名

# `````````````````````````模型加载相关`````````````````````````````
# fast_run: False  # 快速运行整个训练和测试过程，便于查找bug

pretrain_model: t5:t5-base # 除了影响到模型加载，还会影响到使用预训练模型的tokenizer还是自定义的tokenizer
stage: train # test, train, finetune
ckpt_identifier: 2023-01-10-13-31-18-->T5_wow-->wow-->t5:t5-base

# `````````````````````````数据相关```````````````````````````````
dataset: faith_dial # 影响数据集的存放和保存地址
dataset_version: base_ctrl # 使用哪个版本的数据集预处理
dataset_processor: bow_base
dataset_split:
model_processor: hf_seq2seq # 如果使用general_files中的基础模型，需要以“base:”开头，如果使用pl框架，model_processor名需包含“pl.”

pipline_model: t5:t5-base # 如果使用pipeline，那么这里将是pipeline的模型权重名称，可以是 ckpt 路径，也可以是Huggingface的模型名称
pipline_ckpt: 2022-10-08-10-11-53-->t5_baseline_custom-->faith_dial-->t5:t5-base # 如果使用pipeline，那么这里将是pipeline的模型权重名称，可以是 ckpt 路径，也可以是Huggingface的模型名称
pipline_model_processor: hf_custom # 如果 pipline 使用 ckpt，需要此指定使用哪个模型处理文件

force_reload_data: False # True, False # 是否强制重新处理数据，不使用preprocess_data_path加载
decoder_max_length: 128 # 解码器最长长度
encoder_max_length: 256 # 编码器最长长度

train_batch_size: 8 # 训练集的batch大小
valid_batch_size: 8 # 验证集的batch大小
test_batch_size: 8 # 测试集的batch大小

save_total_limit: 0
save_best_model: False # 是否保存最好的模型
save_preprocess_data: False # 是否保存预处理后的数据

# `````````````````````````特殊设置`````````````````````````````
input_shape: c-k-h
target_shape: r
decoder_start_token: <bos>
history_len: 3
use_no_know_label: True
loss: lm_loss
nce_temperature: 0.1
use_pure_sim: False
use_only_sim: False
use_constant_attn: False
lamda: 0.01


additional_special_tokens:
  - <user>
  - <bot>
  - <low-prec>
  - <med-prec>
  - <high-prec>
  - <non-entailed>
  - <neutral>
  - <entailed>
  - <no-first-person>
  - <first-person>
  - <knowledge>
  - <high_know>
  - <use_know>
  - <no_know>
  - <control>
  - <:>

# Section 关于训练相关的参数
eval_metrics:
  - nlg_eval
  # - hf_ppl
  - sent_bleu
#  - hf_google_bleu
  - corpus_bleu
#  - hf_sacrebleu
  - sacrebleu_sent
  - sacrebleu_corpus
  - dist
  - meteor
  - rouge
  - hf_rouge
  - bert_score # 要求Dataset中含有‘generated’和‘bert_score_reference’两个列
#  - hf_bert_score # 要求Dataset中含有‘generated’和‘bert_score_reference’两个列
  - f1_space_split # 要求Dataset中含有‘generated’和‘f1_reference’两个列
  - f1_nlp_split
  - charf
#  - hf_chrf
  - q_squared

# `````````````````````````模型生成相关````````````````````````````
temperature: 1.0
beam_size: 1
top_k: 8
top_p: 0.6
max_generation_length: 128

# `````````````````````````训练流程相关````````````````````````````
max_epochs: 10
lr: 6.25e-5
scheduler: linear # linear, constant, cosine， cosine_w_restarts， polynomial
adafactor: False # 使用AdaFactor还是AdamW优化器
weight_decay: 0.0
warmup_ratio: 0.04 # 优先级高于warmup_steps
warmup_steps: 200
dropout: 0.2

# `````````````````````````特殊参数````````````````````````````

# Section 关于trainer的特定参数
pl_train_args:
  auto_lr_find: False # True, False
  gradient_clip_algorithm: norm
  gradient_clip_val: 3.5
