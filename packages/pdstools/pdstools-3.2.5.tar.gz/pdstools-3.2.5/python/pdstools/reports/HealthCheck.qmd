---
title: "ADM Health Check"
title-block-banner: true
author: "Pega data scientist tools"
date: today
subtitle: > 
  Summary of all ADM Models
format:
  html:
    code-fold: true
    embed-resources: true
    standalone: true
    code-tools: true
    toc: true
    toc-title: Table of Contents
    theme:
        light: flatly
        dark: darkly
jupyter: python3
---
```{python}
#| label: Imports
#| code-fold: true
#| code-summary: Python imports
#| output: false
import logging, sys
logging.disable()
from IPython.display import display, Markdown
def Text(d):
   return display(Markdown(d))
import sys
sys.path.append('..')
from pdstools import datasets, ADMDatamart
from pdstools import cdh_utils
from pdstools import defaultPredictorCategorization
from plotly.offline import iplot
from itables import show, JavascriptFunction
import plotly.express as px
import plotly.graph_objs as go
import polars as pl
import pandas as pd
import numpy as np
from pdstools.utils import pega_template 
import math
```


```{python}
#| tags: [parameters]
#| echo: false

# The kwargs argument is in support of the streamlit app specifically.
# The folder/filename arguments are in support of calling from a command line,
# using the same argument names as the stand-alone model report.
# When neither is passed in, it falls back to the sample data.

name = 'CDH Sample'
filters = dict() # TODO: not used?
kwargs = dict()

title = "Demo Dataset" # pass in customer name here
subtitle = "" # typically used to pass in a date range or other qualification of the data source

datafolder = ""
modelfilename = ""
predictorfilename = ""

include_tables=True
globalQuery = None

```

```{python}
#| echo: false

display(
    Markdown(
        f"""
# {title}

## {subtitle}

"""
    )
)
```


```{python}
#| tags: [initialization]
#| code-fold: true
#| code-summary: Initialization of the datamart class.

# Initialize the class after the parameters have been overwritten.

if len(kwargs)>0: 
    # streamlit call
    datamart = ADMDatamart(**kwargs, include_cols="pyFeatureImportance").fillMissing()
elif len(datafolder) > 0 or len(modelfilename) > 0 or len(predictorfilename) > 0:
    # command line call
    datamart = ADMDatamart(
        path = "." if len(datafolder) == 0 else datafolder, 
        model_filename = "" if len(modelfilename)==0 else modelfilename, 
        predictor_filename = "" if len(predictorfilename)==0 else predictorfilename)
else: 
    # fall back to sample data
    datamart = datasets.CDHSample()
treatment = (
    "Treatment" in datamart.modelData.columns
    and datamart.modelData.schema["Treatment"] != pl.Null
)
    
last_data = (
    datamart.last(strategy='lazy')
    .with_columns(pl.col(pl.Categorical).cast(pl.Utf8))
    .with_columns(
        [
            pl.col(pl.Utf8).fill_null("NA"),
            pl.col(pl.Null).fill_null("NA"),
            pl.col("SuccessRate").fill_nan(0).fill_null(0),
            pl.col("Performance").fill_nan(0).fill_null(0),
            pl.col("ResponseCount").fill_null(0),
            (pl.concat_str("Channel/Direction".split("/"), separator="/")).alias("Channel/Direction"),
        ]
    )
).collect()

if datamart.predictorData is not None:
    datamart_all_columns = datamart.combinedData.columns
else:
    datamart_all_columns = datamart.modelData.columns

channel_overview_columns = [
    col for col in ["Channel", "Direction"] if col in datamart_all_columns
]

standardNBADNames = [
    "Assisted_Click_Through_Rate",
    "CallCenter_Click_Through_Rate",
    "CallCenterAcceptRateOutbound",
    "Default_Inbound_Model",
    "Default_Outbound_Model",
    "Email_Click_Through_Rate",
    "Mobile_Click_Through_Rate",
    "OmniAdaptiveModel",
    "Other_Inbound_Click_Through_Rate",
    "Push_Click_Through_Rate",
    "Retail_Click_Through_Rate",
    "Retail_Click_Through_Rate_Outbound",
    "SMS_Click_Through_Rate",
    "Web_Click_Through_Rate",
]

currentConfigurationNames = datamart.modelData.select(pl.col('Configuration')).unique().collect().to_series(0).to_list()
configurationNamesInStandardNBADModelNames = [(c in standardNBADNames) for c in currentConfigurationNames]
```

This document gives a global overview of the Adaptive models and predictors. It is generated from a Python markdown file in the [Pega Data Scientist Tools](https://github.com/pegasystems/pega-datascientist-tools). This is open-source software and comes without guarantees. Off-line reports for individual
models can be created as well, see [Wiki](https://github.com/pegasystems/pega-datascientist-tools/wiki).

We provide guidance and best practices in the form of bulletted lists of attention points. However these are only generic practices and may or may not be applicable to the specific use case and situation of the implementation.

```{python}
# Start with a global bubble chart. Maybe later replace by
# some ADM metrics, e.g. overall AUC, CTR, some other things.
fig = datamart.plotPerformanceSuccessRateBubbleChart()
fig.layout.coloraxis.colorscale = pega_template.success
fig.for_each_xaxis(lambda xaxis: xaxis.update(showticklabels=True, visible=True))
fig.for_each_xaxis(lambda xaxis: xaxis.update(dict(
        tickmode = 'array',
        tickvals = [50, 60, 70, 80, 90,100],
    )))
fig.update_layout(autosize=True, height=400, title="All ADM Models",
xaxis_title="Model Performance", yaxis_title="Success Rate")
fig.update_coloraxes(showscale=False)
fig.show()
```

::: {.callout-tip}
The [Plotly](https://plotly.com/python/) charts have [user controls for panning,
zooming etc](https://plotly.com/chart-studio-help/zoom-pan-hover-controls/) but
note that these interactive plots do not render well in portals like Sharepoint
or Box. It is preferable to view them from a browser.
:::

# Overview of Channels

In a typical NBAD setup, channels are served by both one channel specific model configuration as well as a cross-channel *OmniAdaptiveModel* configuration.

If a channel has two model configurations with a naming pattern like “Adm_12345678912”, this could indicate the usage of the (no longer recommended) “2-stage model” predictions for conversion modeling, generated by Prediction Studio.

```{python}
framework_usage = 'is being used' if all(configurationNamesInStandardNBADModelNames) else ('is being used with additional configurations' if any(configurationNamesInStandardNBADModelNames) else 'is not being used')

display(
    Markdown(
        f"""
The standard Pega Next Best Action Designer framework defines a number
of standard Adaptive Models for channels. By looking at the names of the
configurations it seems that the framework **{framework_usage}**.
"""
    )
)
```

::: {.callout-tip}
* Look out for channels supported by more than two model configurations, although there may be valid reasons to do so (e.g. different sets of predictors for certain issues)
* Channels with no responses at all
* Channels with no positive feedback
:::

```{python}
channel_overview = (
    datamart.modelData
    # first, take max per model ID
    .group_by(["Configuration", "ModelID"] + channel_overview_columns)
    .agg(
        pl.max("ResponseCount"), 
        pl.max("Positives")
    )
    # then, take sum of model max per configuration
    .group_by(["Configuration"] + channel_overview_columns)
    .agg(
        pl.sum("ResponseCount"), 
        pl.sum("Positives")
    )
    # finally, take the max of the configurations per channel
    .group_by(channel_overview_columns)
    .agg(
        pl.max("ResponseCount").cast(pl.Int32).alias("Responses"), 
        pl.max("Positives").cast(pl.Int32),
        pl.format("{}%", ((pl.max("Positives") * 100 / pl.max("ResponseCount")).round(3))).alias("Base rate"),
        pl.col("Configuration").unique().alias("Supported by Configurations")
    )
    .collect()
)

df = channel_overview.to_pandas()

def get_background_style_channel_overview(values):
    def get_background(value):
        if isinstance(value, int) or isinstance(value, float):
            if value == 0:
                return("background: red")
            elif value < 200:
                return("background: orange")
            else:
                return(None)
        # else:
        #     # warning for > 2 configurations to drive one channel
        #     if len(value) > 2:
        #         return("background: orange")
        #     else:
        #         return(None)
        return(None)

    return([get_background(v) for v in values])

# work in progress..
# df_styled = (
#     df.style
#     .apply(get_background_style_channel_overview, axis=1, subset=["Positives", "Responses"])
# )

show(df)
```


# Overview of the Actions

In a standard setup, the offers/conversations are presented as treatments for actions in a hierarchical structure setup in NBA Designer. Treatments are often channel specific and you would typically expect more unique treatments than there are actions.

Adaptive Models are created per treatment (at least in the default setup) and the recommendation is to stick the default context keys of the models.

```{python}

# TODO consider a different way to create the table (Quarto has better options)
# and include all the same items as the R version. Currently not having all of
# them here.

context_keys= {'Channels':'Channel/Direction', 'Issues':'Issue', 'Groups':'Group','Actions':'Name', 'Treatments':'Treatment'}
value_keys = ['Actions', 'Treatments','Issues', 'Groups', 'Channels']
counts, values = dict(), dict()

for label, column in context_keys.items():
    if column in last_data.columns:
        if label in value_keys:
            datalist = ', '.join(filter(None, (last_data.select(context_keys[label]).to_series().unique().sort().to_list())[:5]))
        else:
            datalist = ''
        n = last_data.select(column).to_series().n_unique()
    else:
        datalist, n = '', 0
    counts[f'Number of {label}'] = [n, datalist]
overview_of_adaptive_models = pd.DataFrame(
    counts, index=['Counts', 'Values']).T

show(overview_of_adaptive_models, columnDefs=[{"className": "dt-left", "targets": "_all"}])
```

::: {.callout-tip}

-   Recommended best practice is to have multiple treatments for an
    action. Too few gives less opportunity for personalization of the
    interactions.
-   Pega Customer Decision Hub deployed in Pega Cloud has limits in
    place that constrain certain elements of the service and its use to
    ensure high quality of service for your team. Where relevant, these
    limits are used in the guidance in this health check. See [Service
    and data health limits for Pega Customer Decision Hub on Pega
    Cloud](https://docs-previous.pega.com/pega-customer-decision-hub-user-guide/87/service-and-data-health-limits-pega-customer-decision-hub-pega-cloud).
:::

## Success Rates per Channel

Showing the current success rate of the treatments. Different channels usually have very different success rates. Just showing the top 20 here and limiting to the propositions that have received at least 100 responses (the rates reported by the models are unreliable otherwise).

::: {.callout-tip}
- Look out for propositions that stand out, having a far higher success rate than the rest. Check with business if that is expected.

- Variation in the set of offered propositions across customers is also an important metric but not one that can be derived from the Adaptive Model data - this requires analysis of the actual interactions.
:::

```{python}
facet = "Channel/Direction"
hover_columns = [col for col in  ["Issue", "Group", "Name"] if col in datamart_all_columns]
if treatment:
    hover_columns += ["Treatment"]
df = (
    last_data.lazy()
    .with_columns(pl.concat_str(facet.split("/"), separator="/").alias(facet))
    .with_columns(pl.col(pl.Categorical).cast(pl.Utf8))
)
df = (
    df.filter(pl.col("ResponseCount") > 100)
    .select(hover_columns + ["ModelID", "Channel/Direction","SuccessRate"])
    .with_columns(pl.col("SuccessRate").round(4))
    .sort(["Channel/Direction", "SuccessRate"], descending=True)
    .group_by(["Channel/Direction"])
    .head(20)
    .collect()
).to_pandas(use_pyarrow_extension_array=True)

hover_data = {
    "SuccessRate": ":.2%",
}
for col in hover_columns:
    hover_data[col] = ":.d"


facet = "Channel/Direction"
facet_col_wrap = 3
fig = px.bar(
    df.sort_values(["Channel/Direction", "SuccessRate"]),
    x="SuccessRate",
    y="ModelID",
    color="SuccessRate",
    facet_col=facet,
    facet_col_wrap=facet_col_wrap,
    template="pega",
    text="Name",
    title="Proposition success rates <br><sup>Issue/Group/Name/Treatment</sup>",
    hover_data=hover_data,
)
fig.update_xaxes(tickformat=",.2%")
fig.update_yaxes(matches=None, showticklabels=False, visible=False).update_xaxes(
    matches=None,
).update_traces(textposition="inside")
fig.for_each_annotation(
    lambda a: a.update(text=a.text.replace("Channel/Direction=", ""))
)
fig.update(layout_coloraxis_showscale=False)

unique_count = (
    datamart.last(strategy="lazy")
    .with_columns(pl.concat_str(facet.split("/"), separator="/").alias(facet))
    .select(facet)
    .collect()
    .to_series()
    .n_unique()
)

height = 200 + (math.ceil(unique_count / facet_col_wrap) * 250)

fig.update_layout(autosize=True, height=height)

display(fig)
```

## All Success Rates
Interactive chart with all success rates.

```{python}

# TODO put the percentage itself in the tree cells just like in the R version
#

levels = [col for col in ["Configuration",'Channel', 'Direction', 'Issue', 'Group', "Name", "Treatment"] if col in datamart_all_columns]
fig = datamart.plotTreeMap(color_var="SuccessRate",
                     group_by_col=None,
                     levels=levels, 
                     colorscale=pega_template.success,
                     query=pl.col("ResponseCount")>100,)
fig
```

## Success Rates over Time
Showing how the overall channel success rates evolved over the time that the data export covers. Split by Channel and model configuration. Usually there are separate model configurations for different channels but sometimes there are also additional model configurations for different outcomes (e.g. conversion) or different customers (e.g. anonymous).

::: {.callout-tip}
- There shouldn’t be too sudden changes over time
:::

```{python}
by = "Channel/Direction"
facet = "Configuration"
fig = datamart.plotOverTime('SuccessRate', by=by, facets=facet, facet_col_wrap=2, query=pl.col("ResponseCount") > 100)
fig.update_yaxes(matches=None)
fig.for_each_yaxis(lambda yaxis: yaxis.update(showticklabels=True))
unique_count = datamart.modelData.with_columns(pl.concat_str(facet.split("/"), separator="/").alias(facet)).select(facet).collect().unique().shape[0]
height = 200 + (math.ceil( unique_count / 2) * 250)
fig.update_layout(autosize=True, height=height)
fig.for_each_annotation(
    lambda a: a.update(text=a.text.replace(f"{facet}=", ""))
)
fig.for_each_xaxis(lambda xaxis: xaxis.update(showticklabels=True))
fig.show()
```

# Overview of Adaptive Models

```{python}
n_unique_models = len(last_data.select('ModelID').unique()) # TODO or uniqueN ?

display(
    Markdown(
        f"""
There are a total of {n_unique_models} unique models in the latest snapshot.
"""
    )
)
```

The standard configuration is to have one model per treatment.

```{python}
if include_tables:
    # TODO review table,
    # preferably including some simple guidance by applying colors

    model_overview = datamart.model_overview.to_pandas(use_pyarrow_extension_array=True)
    show(model_overview)
else:
    print('Please refer to the `model_overview` tab in the included Excel file.')
```

## Model Performance 

### Model Performance vs Action Success Rates (the Bubble Chart)

This “Bubble Chart” - similar to the standard plot in the ADM reports in Pega - shows the relation between model performance and proposition success rates. In addition, the size of the bubbles indicates the number of responses.

::: {.callout-tip}

-   Bubbles stacked up against the left-hand vertical axis represent
    actions/treatments for which the models are not predictive. These
    models may be still be ramping up, or they may not have predictive
    enough features to work with: consider if new/better predictors can
    be added.

-   Charts should not be empty or contain only a few bubbles. Such
    charts may represent channels or configurations not (or no longer).

-   Bubbles at the bottom of the charts represent propositions with very
    low success rates - they may not be compelling enough.

-   In an ideal scenario you will see the larger bubbles more on the
    top-right, so more volume for propositions with higher success rates
    and better models.

-   There should - roughly - be a positive correlation between success
    rate and performance per channel.

-   There should be a positive correlation between responses and
    performance - also per channel.

-   There should be variation in response counts (not all dots of equal
    size)

-   For small volumes of good models, see if the engagement rules in the
    Decision Strategy are overly restrictive or reconsider the
    arbitration of the propositions so they get more (or less) exposure.
:::

```{python}
facet_col_wrap=2
facet = 'Configuration/Channel/Direction'
fig = datamart.plotPerformanceSuccessRateBubbleChart(facets=facet,facet_col_wrap=facet_col_wrap)
fig.layout.coloraxis.colorscale = pega_template.success
fig.for_each_xaxis(lambda xaxis: xaxis.update(showticklabels=True, visible=True))
fig.for_each_xaxis(lambda xaxis: xaxis.update(dict(
        tickmode = 'array',
        tickvals = [50, 60, 70, 80, 90,100],
    )))
height = 250 + (math.ceil( len(fig.layout.annotations) / facet_col_wrap) * 270)
fig.update_layout(autosize=True, height=height, title=None)

fig.for_each_annotation(
    lambda a: a.update(text=a.text.replace(f"{facet}=", ""))
)
fig.update_layout(font=dict(size=10))

fig.for_each_annotation(lambda a: a.update(text="<br> ".join(a.text.split("/", 1))))
fig.update_coloraxes(colorbar_len= 1 / math.ceil( len(fig.layout.annotations) / facet_col_wrap))
fig.update_yaxes(tickformat=",.2%")
fig.show()
```

### Model Performance over Time
Showing how the model performance evolves over time. Note that ADM is by default configured to track performance over all time. You can configure a window for monitoring but this is not commonly done.

Aggregating up to Channel and splitting by model configuration.

::: {.callout-tip}
- No abrupt changes but gradual upward trend is good
:::

```{python}
facet = "Configuration"
modelperformance = datamart.plotOverTime('weighted_performance', by="Channel/Direction", facets=facet, facet_col_wrap=2)

unique_count = datamart.modelData.with_columns(pl.concat_str(facet.split("/"), separator="/").alias(facet)).select(facet).collect().unique().shape[0]
height = 200 + (math.ceil( unique_count / 2) * 250)
modelperformance.update_layout(autosize=True, height=height)
modelperformance.for_each_annotation(
    lambda a: a.update(text=a.text.replace(f"{facet}=", ""))
)

modelperformance.for_each_xaxis(lambda xaxis: xaxis.update(showticklabels=True))

modelperformance.show()
```

### Model performance of all the actions

Using an interactive treemap to visualize the performance. Lighter is better, darker is low performance.

It can be interesting to see which issues, groups or channels can be better predicted than others. Identifying categories of items for which the predictions are poor can help to drive the search for better predictors, for example.

```{python}
fig = datamart.plotTreeMap(
    color_var="performance_weighted", 
    group_by_col= None, 
    levels=levels)
fig.update_layout(
    title="Model Performance", 
    showlegend=False)

fig.show()
```

### Response counts for all the actions

Using an interactive treemap to visualize the response counts.
Different channels will have very different numbers but within one channel the relative differences in response counts give an indication how skewed the distribution is.

Warning : Currently treemap calculates mean response count moving upwards in the hierarchy. 

::: {.callout-tip}

If there are actions that have a much higher response count than the rest see why that is. Possibly they are levered up for valid business reasons.
:::


```{python}
fig = datamart.plotTreeMap(
    color_var="responsecount", 
    levels=levels, 
    colorscale=pega_template.negative_positive)

fig.update_layout(
    title="Response Counts", 
    showlegend=False)

fig.show()
```

# Analysis of Predictors

```{python}
#| output: asis
#| echo: false

if datamart.predictorData is None:
    display(Markdown(
"""
::: {.callout-important}
Predictor Data is not available. All the below analyses based on predictor data
will be empty.
:::
"""))
```

This analysis focuses on finding which are top predictors that are driving the models.

The predictors are categorized (by color) by the “source”. By default this takes just the first part before the dot, so this typically distinguishes between Customer, Account, IH and parameterized (Param.) predictors. You can customize this to add patterns to identify for example external scores.

## Number of Predictors per model configuration 

Both active and inactive predictors.

Note that the total number of predictors in the model data does not always equate the data from the more detailed view split by category below.

```{python}
#| error: true

if datamart.predictorData is not None:
    if include_tables:

        # TODO: see if we can highlight table cells like in the R version
        # to highlight best practices

        predictors_per_configuration = datamart.predictors_per_configuration.to_pandas(use_pyarrow_extension_array=False)
        show(predictors_per_configuration)
    else:
        print('Please refer to the `predictors_per_configuration` tab in the included Excel file.')
else:
    Text("No predictor data")
```

## Number of Predictors per Predictor Category

Split by category (defaults to the string before the first dot, can be overridden when reading the data).

The numbers here can differ from the totals above, these ones are leading.

TODO: include a table like in the R version

::: {.callout-tip}
- Total number of predictors per model 200 - 700 to stay within service limits
- There should be some “IH” predictors but no more than ca 100 of them
- No more than a few dozen Param predictors
- Consistency in the numbers across configurations
:::

## Predictor Importance across all models per configuration

Box plots of the predictor importance. Predictor importance is using the 
univariate predictor performance.

::: {.callout-tip}
* You expect most predictors to have a spread in the performance range, doing better for some actions than for others
* Predictors only showing as a single bar (no range) are suspicious
* A variation of predictors from different categories in the top 30
* A min/max of the univariate AUC performance somewhere between 55 and 75
:::

```{python}
# TODO: Optionally feature importance if available in the datamart
# drop our own calculations of feature importance. Would be nice to
# be able to toggle in the graph (with a tab)
```

```{python}
#| error: true
to_plot = (
    "FeatureImportance"
    if "FeatureImportance" in datamart_all_columns
    else "Performance"
)
if datamart.predictorData is not None:
    figs = datamart.plotPredictorPerformance(
        top_n=30, facets="Configuration", separate=True, active_only=False,to_plot=to_plot
    )
    if not isinstance(figs, list):
        figs = [figs]
    for fig in figs:
        fig.update_traces(width=0.3)
        fig.update_layout(font=dict(size=10), height=700)
        fig.layout.xaxis.tickformat = ".d"

        fig.show()
else:
    Text("Predictor data is not available.")
```

## Importance by Predictor Category 
Aggregating up to the category of the predictors. This gives a view at a glance of how well e.g. interaction history, external model scores or contextual data are doing overall.

### Predictor Category performance per Channel/Direction/Issue 

```{python}
#| error: true

if datamart.predictorData is not None:
    facets = 'Configuration/Channel/Direction'
    facet_col_wrap = 3
    fig = datamart.plotPredictorCategoryPerformance(facets=facets, facet_col_wrap=facet_col_wrap)

    fig.update_layout(font=dict(size=10))
    fig.for_each_annotation(
        lambda a: a.update(text=a.text.replace(f"{facets}=", ""))
    )
    fig.for_each_annotation(lambda a: a.update(text=" <br> ".join(a.text.split("/"))))

    facet_list = []
    for data in fig.data:
        facet_list.append(data["yaxis"][1:])
    facet_count = len(set(facet_list))

    height = 200 + (math.ceil( facet_count/ facet_col_wrap) * 250)
    fig.update_layout(autosize=True, height=height)
    fig.show() 
else:
    Text("Predictor data is not available.")

```

### Relative Predictor Category importance per Configuration
Although the same could be achieved using the standard **plotPredictorImportance** method, now that we only split by Configuration this allows for a more compact visualization using a stacked bar chart.

```{python}

''' By dividing a predictor category's weighted performance to the sum of all predictor categories weighted performance in a configuration, creates a plot that displays relative importance of categories in a configuration.
Changes the Predictor performance range from 50-100 to 0-100 in order to increase visibilty of performance differences among categories.'''

if datamart.predictorData is not None:
    fig = datamart.plotPredictorContribution()
    height = 200 + (math.ceil(len(datamart.modelData.select(pl.col("Configuration").unique()).collect()['Configuration'].to_list()) / 2) * 50)
    fig.update_layout(height=height)
    fig.update_yaxes(
        automargin=True,
        dtick=1,
    )
    fig.show()
else:
    Text("Predictor data is not available")
```

## Bad predictors across all models
See if there are predictors that are just always perform poorly.

::: {.callout-tip}
- Predictors that consistently perform poorly could potentially be removed.
- Be sure to check for data problems
- Note we advise to be careful with predictor removal. Only remove if there is clearly no future value to other propositions as well or if there is always a related predictor that performs better.
:::

```{python}
# weighted performance
if datamart.predictorData is not None:
    if include_tables:
        responses_column_index = datamart.bad_predictors.columns.index("Response Count")
        bad_predictors = datamart.bad_predictors.to_pandas(
            use_pyarrow_extension_array=False
        )

        show(
            bad_predictors,
            scrollX=True,
            columnDefs=[
                {
                    "className": "dt-left",
                    "targets": [responses_column_index],
                    "createdCell": JavascriptFunction(
                        """
    function (td, cellData, rowData, row, col) {
        if (cellData < 200) {
            $(td).css('color', 'red')
        }
    }
    """
                    ),
                }
            ],
        )
    else:
        print("Please refer to the `bad_predictors` tab in the included Excel file.")
else:
    Text("Predictor data is not available")
```

## Number of Active and Inactive Predictors
Showing the number of active and inactive predictors per model.

::: {.callout-tip}

- We expect a few dozen active predictors for every model instance

:::

```{python}
if datamart.predictorData is not None:
    facets= ["Configuration"]
    fig = datamart.plotPredictorCount(facets = facets)

    height = 250 + (math.ceil(len(fig.layout.annotations) / 2) * 270)
    fig.update_layout(autosize=True, height=height)
    fig.update_yaxes(categoryorder="array", automargin=True, dtick=1)
    fig.for_each_annotation(
        lambda a: a.update(text=a.text.replace(f"{facets[0]}=", ""))
    )
    fig.show()
else:
    Text("Predictor data is not available")
```

## Predictor Performance across Propositions
A view of predictor performance across all propositions, ordered so that the best performing predictors are at the top and the best performing propositions are on the left. Green indicates good performance, red means more problematic - either too low or too good to be true.

```{python}
index_cols = [col for col in ['Issue', 'Group', "Name", "Treatment"] if col in datamart_all_columns]
if datamart.predictorData is not None:
    unique_configurations = datamart.combinedData.collect().get_column("Configuration").unique().to_list()
    for conf in unique_configurations:
        try:
            fig = datamart.plotPredictorPerformanceHeatmap(top_n=20, 
                                                    by="/".join(index_cols),
                                                    query = pl.col("Configuration")==conf,
                                                    tickangle=45
                                                    )
            fig.update_layout(
                xaxis={
                    "tickmode": "array",
                    "tickvals": fig.data[0]["x"],
                    "ticktext": [
                        "..." + tick[-25:] if len(tick) > 25 else tick for tick in fig.data[0]["x"]
                    ],
                },
                font=dict(size=8),
                title=f"Top predictors over {conf}"
            )

            fig.show()
            
        except:
            print(f"Plot was not drawn for {conf} because of an error")    #TODO: add error message instead of typing "an error"
else:
    Text("Predictor data is not available")
```

## Missing values
If a predictor is low performing: are there too many missing values? This could point to a technical problem
Missing % is number of missing vs all responses, really just a filter on model data
This TreeMap only shows the fields that have any missing values.

```{python}
if datamart.predictorData is not None:
    path =  [col for col in ["Configuration", "PredictorCategory", "PredictorName"] if col in datamart_all_columns]
    gb_cols = path
    path = [px.Constant("All Models")] + path 

    missing = datamart.last(table = "combinedData").filter(pl.col("PredictorName") != "Classifier").group_by(gb_cols).agg(
        pl.col("BinResponseCount")
        .where(pl.col("BinSymbol") == "MISSING")
        .sum()
        .alias("MissingCount"),
        pl.sum("BinResponseCount").alias("BinResponseCount")
    ).with_columns(
        (pl.col("MissingCount") / pl.col("BinResponseCount")).alias("Percentage without responses")
    ).filter((~pl.col("Percentage without responses").is_nan()))

    hover_data = {
        "Percentage without responses": ":.2%",
    }

    fig = px.treemap(
        missing.to_pandas(),
        path=path,
        color="Percentage without responses",
        template="pega",
        hover_data=hover_data,
        title="Missing Data in Adaptive Models",
    )

    fig.layout.coloraxis.colorscale =  pega_template.positive_negative

    fig.show()
else:
    Text("Predictor data is not available")

```

## Residuals

TODO: similar to analysis of the MISSING bin we can do something for the
Residual bin - the bin in categorical predictors that acts as an overflow
bin. Too much in there is not good. Also, the sheer number of distinct
values could be good to analyse, but we don't have that exactly.

# Responses

In the sections below we check which of these models have reached certain reliability (or “maturity”) thresholds. This is based on heuristics on both the number of positives (> 200 considered mature) and performance.

## Empty and Immature Models

All below lists are guidance. There should be just a small percentage of immature or empty models overall. Having no or just 1 active predictor is very suspicious

### Models that have never been used

These models have no responses at all: no positives but also no negatives. The models for these actions/treatments exist, so they must have been created in the evaluation of the actions/treatments, but they were never selected to show to the customer, so never received any responses.

Often these represent actions that never made it into production and were only used to test out logic. But it could also be that the response mechanism is broken. It could for example be caused by outcome labels that are returned by the channel application not matching the configuration of the adaptive models.

```{python}
if include_tables:
    zero_response = datamart.zero_response.to_pandas(use_pyarrow_extension_array=True)

    if zero_response.shape[0] >0:
        print(f"There are {zero_response.shape[0]} models with zero response")
        show(zero_response, scrollX = True)
    else:
        print("All models have received at least 1 response.")
else:
    print('Please refer to the `zero_response` tab in the included Excel file.')
```

```{python}
#| output: asis

if include_tables:
    zero_positives = datamart.zero_positives.to_pandas(use_pyarrow_extension_array=True)
    if zero_positives.shape[0] >0:
        print("### Models that have have been used but never received a positive response")
        show(zero_positives, scrollX = True)
    else:
        print("All models have received at least 1 positive Response.")
else:
    print('Please refer to the `zero_positives` tab in the included Excel file.')
```

### Models that are still in an immature phase of learning

These models have received at least one positive response but not enough yet to be qualified to be fully  “mature” - a concept that matters especially for outbound channels. 
These actions are typically new and still in early phases of learning. 
We show the “reach” of these actions as the percentage of the population that would be selected by the standard maturity capping algorithm in the NBA framework (which selects 2% for new models and 100% for models with 200 or more positive responses


```{python}
# if include_tables:
#     reach = datamart.reach.to_pandas()
#     reach.style.format({
#         'Reach': '{:,.2%}'.format})
#     show(reach, scrollX=True)
# else:
#     print('Please refer to the `reach` tab in the included Excel file.')

```


```{python}
#| output: asis
if include_tables:
    min_perf_df = datamart.minimum_performance.to_pandas(use_pyarrow_extension_array=True)
    if min_perf_df.shape[0] >0:
        print('''### Models that have received sufficient responses but are still at their minimum performance 
    These models also have received over 200 positives but still show \n
    the minimum model performance. This could be an indication of data problems, \n
    or not having the right predictors but may also be caused by technical aspects \n
    like the order of the responses to the model. \n
        ''')
        show(min_perf_df, scrollX = True)
    else:
        print("All models with over 200 positive responses are above minimum performance")
else:
    print('Please refer to the `minimum_performance` tab in the included Excel file.')
```

## Number of Empty/Immature Models over time 

In the analysis below we count the number of models in each of the groups analysed before and show how that \ncount changes over time. The expectation is that the number of “non-empty” models increases steadily and the other lines are more or less stable.\n
Empty is defined as having no responses at all. Immature is defined as having < 200 positives, and no performance means model performance is still the initial 0.5 value while having matured already according to the definition.

::: {.callout-tip}
- Empty models shouldnt be increasing too much
- Good models (AUC 55-80) should increase or at least not decrease
- Good models should be much higher than problem kids
:::

```{python}
by= ["SnapshotTime", "Channel", "Direction"]
df = (
    datamart.modelData
    .with_columns(pl.col(pl.Categorical).cast(pl.Utf8))
    .with_columns(pl.col(pl.Utf8).fill_null("Missing"))
    .group_by(by)
    .agg(
        [
            (pl.col("Positives") >= 200).sum().alias("Mature Models"),
            (pl.col("ResponseCount")==0).sum().alias("Empty Models"),
            (pl.col("Positives")==0).sum().alias("Models w/o Positives"),
            ((pl.col("Positives") > 0) & (pl.col("Positives") < 200 )).sum().alias("Immature Models"),
            (pl.col("Performance")==0.5).sum().alias("Models w/o Performance"),
            (pl.col("ResponseCount")!=0).sum().alias("Number of non-empty Models")
        ]
    )
    .sort(["Channel","Direction","SnapshotTime"], descending=True)
)
facet_col_wrap=3
facet = "Channel/Direction"
df = df.with_columns(pl.concat_str(facet.split("/"), separator="/").alias(facet)).with_columns(pl.col(facet).cast(pl.Utf8).fill_null("NA"))
df_pd = df.collect().to_pandas()
y = df_pd.iloc[:,len(by):].columns.tolist() #slice `df.columns` to get this easily
fig = px.line(df_pd,
    x="SnapshotTime",
    y=y,
    facet_col = facet,
    title="Immature and Empty Models over Time",
    template="pega",
    facet_col_wrap=facet_col_wrap,
    )
fig.for_each_annotation(
    lambda a: a.update(text=a.text.replace(f"{facet}=", ""))
)
unique_count = len(fig.layout["annotations"])
height = 200 + (math.ceil( unique_count / facet_col_wrap) * 250)
fig.update_layout(
    autosize=True, 
    height=height, 
    xaxis_title="", 
    yaxis_title="Number of Models")

fig.show()
```

## Number of Responses over time 


```{python}
facets = "Configuration"
facet_col_wrap = 2
response_counts = datamart.plotOverTime('ResponseCount', by="Channel/Direction", facets=facets, facet_col_wrap=facet_col_wrap, mode="Cumulative")

unique_count = datamart.modelData.select(facets).unique().collect().shape[0]
height = 200 + (math.ceil( unique_count / 2) * 250)
response_counts.update_layout(autosize=True, height=height)
response_counts.for_each_annotation(
    lambda a: a.update(text=a.text.replace(f"{facets}=", ""))
)
response_counts.show()
```

# Which Models drive most of the Volume

## Analysis of skewness of the Responses
Showing the cumulative response count vs the number of models. Is there a larger percentage of models that take the vast majority of the responses?

If this line strongly deviates from the diagonal it means that relatively few models drive the majority of the responses.

In the left-hand plot we look at all responses, which really means that we are looking at “impressions” mostly. The right-hand plot looks at just the positives. Typically, the positives are driven more strongly by the models so often you see more skewness in that one.

However very skewed results may be caused by prioritization elements like levers and weights and can be a reason to check in with business and verify that this is expected.

::: {.callout-tip}
- Area under this curve should be > 0.5 and perhaps more towards 1 - most of the responses driven by relatively few actions
:::


::: {layout-ncol=2}
```{python}
#| output: asis

fig = datamart.plotResponseGain()

fig.update_layout(title="Cumulative Responses", height=300, width=300)
fig.add_shape(type="line", x0=0, y0=0, x1=1, y1=1, line=dict(color="grey", dash="dash"))
fig.update_yaxes(scaleanchor="x", scaleratio=1)

cdh_utils.legend_color_order(fig).show()
```

```{python}
#| output: asis

# TODO: I don't think this is doing what we want
# this seems to filter, but we want to plot positives
# rather than responses

fig = datamart.plotResponseGain(query=pl.col("Positives") > 0)

fig.update_layout(title="Cumulative Positives", height=300, width=300)
fig.add_shape(type="line", x0=0, y0=0, x1=1, y1=1, line=dict(color="grey", dash="dash"))
fig.update_yaxes(scaleanchor="x", scaleratio=1)

cdh_utils.legend_color_order(fig).show()
```

:::

## Models with largest number of responses (positive or negative)

Zooming in into the models that drive most of the responses, here we list the top 20 models with the highest number of responses.

```{python}
subset = ['Configuration', 'Issue', 'Group', 'Name', 'Channel', 'Direction']
facet = '/'.join([col for col in subset if col in datamart_all_columns])

for split_facet in facet.split("/"):
    last_data = last_data.with_columns(
        pl.col(split_facet).cast(pl.Utf8).fill_null("NA")
    )
last_data = last_data.with_columns(pl.concat_str(facet.split("/"), separator="/").alias(facet))
response_counts = last_data.group_by([facet] + facet.split("/")).agg(
    [
        pl.sum("ResponseCount").alias("all_responses"),
        pl.sum("Positives").alias("positive_sum")
        ]
    )

all_responses = response_counts.sort("all_responses", descending=False).tail(20)
hover_data = {
    facet: False
}
for col in facet.split("/"):
    hover_data[col] = ":.d"

possible_color_vars = ['Channel', 'Issue', 'Name']
color = next((col for col in possible_color_vars if col in datamart_all_columns), None)

fig = px.bar(all_responses.to_pandas(use_pyarrow_extension_array=True), x ="all_responses", y=facet, color=color, title="Top 20 Highest Responses", template="pega", text=facet, hover_data=hover_data)
fig.update_yaxes(matches=None, showticklabels=False, visible=False).update_traces(textposition="inside")
fig.for_each_annotation(
    lambda a: a.update(text=a.text.replace(facet, ""))
)
fig.update_layout(
    yaxis={'categoryorder':'total ascending'}
)    

cdh_utils.legend_color_order(fig).show()
```

## Models with largest number of positive responses. 
And these are the 20 models with the largest number of positives.

```{python}
positives = response_counts.sort("positive_sum", descending=True).head(20)
fig = px.bar(positives.to_pandas(use_pyarrow_extension_array=True), x ="positive_sum", y=facet, color=color, title="Top 20 Highest Positives", template="pega", text=facet, hover_data=hover_data)
fig.update_yaxes(matches=None, showticklabels=False, visible=False).update_xaxes(matches=None).update_traces(textposition='inside')
fig.for_each_annotation(
    lambda a: a.update(text=a.text.replace(facet, ""))
)
fig.update_layout(yaxis={'categoryorder':'total ascending'})

cdh_utils.legend_color_order(fig).show()
```

## Analysis of Performance vs Volume
Is most volume driven by models that have a good predictive performance? Ideally yes, so the targeting of the customers is optimal. If a lot of volume is driven by models that are not very predictive, this could be a reason to look into the available predictor data.

The plot below shows this relation. Horizontally the model performance (the AUC, ranging from 50 to 100 as Pega usually scales this), descretized into a number of ranges, and vertically the percentage of responses.

A lot of volume on the first bins, where the performance is minimal, means that a lot of immature models are used. This is sub-optimal in terms of targeting. Ideally there is a smooth curve with a peak in the 60-80 range of AUC. Much higher AUC’s are possibly indicative of immature models or even outcome leakers (although that is effectively prevented by the standard delayed learning pattern). AUC’s below 60 are not uncommon but should be investigated - consider different predictors or outcomes.
```{python}
#| error: true
to_plot = "Performance"
df = (
    datamart.modelData.with_columns(pl.col(to_plot) * 100)
    .group_by([to_plot, "Channel", "Direction"])
    .agg(pl.sum("ResponseCount"))
    .with_columns(pl.col(to_plot).round(2))
    .collect()
)

breaks = [percentile for percentile in range(50, 100, 3)]
df = df.with_columns(pl.col("Performance").cut(breaks=breaks).alias("PerformanceBin"))

grouped = df.group_by(["Channel", "PerformanceBin"]).agg(
    pl.sum("ResponseCount"), pl.min(to_plot).alias("break_label")
)
out = (
    grouped.sort(["Channel", "break_label"])
    .select(
        [
            pl.col("Channel").cast(pl.Utf8),
            "PerformanceBin",
            "ResponseCount",
            pl.col("ResponseCount").sum().over("Channel").alias("sum"),
        ]
    )
    .with_columns([(pl.col("ResponseCount") / pl.col("sum")).alias("Volume")])
)

fig = px.bar(
    out,
    x="PerformanceBin",
    y="Volume",
    color="Channel",
    template="pega",
    barmode="overlay",
)
for bar in fig.data:
  bar.visible = "legendonly"

channels = out["Channel"].unique()
for channel_num, channel in enumerate(channels):
    channel_df = out.filter(pl.col("Channel") == channel) 
    fig.add_traces(go.Scatter(x = channel_df["PerformanceBin"], y=channel_df["Volume"], line_shape = 'spline', marker_color = fig.data[channel_num].marker.color, name=channel))


fig.update_yaxes(tickformat=",.0%")
fig.update_layout(
    title="Performance vs Volume",
    xaxis_title="Model Performance",
    yaxis_title="Percentage of Responses"
)

cdh_utils.legend_color_order(fig).show()
```

## Positives vs. Number of Models
Ideally, all models have received plenty of responses which will make them “mature” and makes sure they are as predictive as possible.

Often we see that there is a significant percentage of models that are still relatively new and have not received much feedback (yet). Below graph shows the percentages of models that have fewer than 200 positives.

Having many on the left-hand side (with very low or perhaps no positives) may or not be a problem. The models may still be there in the datamart but might represent actions/treatments that are not active. 

```{python}
fig = datamart.plotModelsByPositives()

fig.update_layout(
    title="Positives vs Number of Models",
    xaxis_title="Number of Positives",
    yaxis_title="Percentage of Models"
)

fig = cdh_utils.legend_color_order(fig)
fig.show()
```

# Propensity Analysis
The distribution of propensities returned by the models is yet a different angle.

Higher propensities clearly indicate the offers are more attractive - people apparenty click/accept/convert more often.

## Success Rate Distribution

TODO: implement like in the R version of the health check

## Propensity Distribution

In a more emphathetic setup, you would expect that the distribution of the propensities leans towards the right-hand side: more volume to more attractive offers, although the relation is of course more complex, we are not just blindly pushing the offers with the highest success rates, but take a personalized approach.

Often however, multiple factors are included in the prioritization, changing this picture.

Note that the propensity bins are not of equal width. Propensities are typically very low so with an equal width distribution, almost all volume would be in the first bins. The binning here is based on (roughly) equal volume across all data.

So when one of the graphs shows more volume on the left, that is to be interpreted as relative to the other graphs.

```{python}
#| error: true
to_plot = "Propensity"
if to_plot == "Propensity" and to_plot not in datamart.predictorData.columns:
    to_plot = "BinPropensity"
df = (
    datamart.combinedData.filter(pl.col("PredictorName") != "Classifier")
    .group_by([to_plot, "Channel", "Direction"])
    .agg(pl.sum("BinResponseCount"))
    .with_columns(pl.col(to_plot).round(4).cast(pl.Float64))
    .collect()
)
color_col = "Channel"
smallest_bin = 0

for color in df.get_column(color_col).unique().to_list():
    color_df = df.filter(pl.col(color_col) == color)
    propensity_list = list(
        set(
            color_df.select(to_plot)
            .fill_null(0)
            .fill_nan(0)
            .filter(pl.col(to_plot) > 0)
            .get_column(to_plot)
            .to_list()
        )
    )
    if len(propensity_list) > 0:
        if np.percentile(propensity_list, 10) > smallest_bin:
            smallest_bin = np.percentile(propensity_list, 10)
            cut_off_value = np.percentile(
                propensity_list, [percentile for percentile in range(0, 101, 5)]
            )

df_pl = df.with_columns(
    pl.col(to_plot)
    .fill_null(0)
    .fill_nan(0)
    .cut(breaks=cut_off_value)
    .alias(f"{to_plot}_range")
)

grouped = df_pl.group_by(["Channel", f"{to_plot}_range"]).agg(
    pl.sum("BinResponseCount"), pl.min(to_plot).alias("break_label")
)

out = (
    grouped.sort(["Channel", "break_label"])
    .select(
        [
            "Channel",
            f"{to_plot}_range",
            "break_label",
            "BinResponseCount",
            pl.col("BinResponseCount").sum().over("Channel").alias("sum"),
        ]
    )
    .with_columns([(pl.col("BinResponseCount") / pl.col("sum")).alias("Responses")])
)

out = out.sort(["Channel", "break_label"])
fig = px.bar(
    out,
    x=f"{to_plot}_range",
    y="Responses",
    color=color_col,
    template="pega",
    barmode="overlay",
)
fig.update_yaxes(tickformat=",.0%")

fig.show()
```

## Propensity Thresholding

TODO: Implement like in the R version

# Appendix - all the models

A list of all the models is written to a file so a script can iterate over all models and generate off-line model reports for each of them.

Generally you will want to apply some filtering, or do this for specific models only. This can be accomplished in either this script here, or by editing the generated file.

TODO: reconsider this

```{python}
if include_tables:
    responses_column_index = datamart.appendix.columns.index("Responses")
    appendix = datamart.appendix.to_pandas(use_pyarrow_extension_array=True)
    show(
        appendix,
        columnDefs=[
            {
                "targets": [responses_column_index],
                "createdCell": JavascriptFunction(
                    """
function (td, cellData, rowData, row, col) {
    if (cellData < 200) {
        $(td).css('color', 'red')
    }
}
"""
                ),
            }
        ],
    )
else:
    print("Please refer to the `appendix` tab in the included Excel file.")

```